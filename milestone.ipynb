{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  id  score    link_id                        author  \\\n0         t1_gl0hhsq      1  t3_l6a44e  Io99IHkg-4QzX6xbKwbte0cuzp4=   \n1         t1_gmd0xrl      1  t3_lehibh  1UBdU9GQvCnnXQHAcYaG1uL9V_U=   \n2         t1_gggg1ed      1  t3_kgocvo  Io99IHkg-4QzX6xbKwbte0cuzp4=   \n3         t1_g7zggfh      1  t3_j6n57d  EA1r-K5p_lVBLesLhCFRrKOPN-I=   \n4         t1_fn060jg     26  t3_fyheuv  _aeNuqWD_AT5JIfooWYpKiZR8qg=   \n...              ...    ...        ...                           ...   \n27997318  t1_f8sotb4      1  t3_e20bxb  EA1r-K5p_lVBLesLhCFRrKOPN-I=   \n27997319  t1_eczq4gz      1  t3_abbin1  Pkz1m3vsliYpbnltUnWaQkPFLEo=   \n27997320  t1_f240cz6      0  t3_dbujdv  NfwullBPKgqUPvj_Qr6RPnH1hrI=   \n27997321  t1_em778f3      2  t3_bj2bnd  kj12hcxGWPd3LxjKCNpoPFTDNBQ=   \n27997322  t1_efpy2wq      1  t3_amz190  vmXxzo7c6AP-ZOKZyiO3gxnxoOE=   \n\n               subreddit  created_utc  \n0         wallstreetbets   1611788340  \n1         RedditSessions   1612683157  \n2                  memes   1608454223  \n3                 videos   1602058834  \n4                    nfl   1586536065  \n...                  ...          ...  \n27997318  Showerthoughts   1574785778  \n27997319       AskReddit   1546315274  \n27997320  DestinyTheGame   1569942007  \n27997321           funny   1556683776  \n27997322           memes   1549283237  \n\n[27997323 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>link_id</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>created_utc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t1_gl0hhsq</td>\n      <td>1</td>\n      <td>t3_l6a44e</td>\n      <td>Io99IHkg-4QzX6xbKwbte0cuzp4=</td>\n      <td>wallstreetbets</td>\n      <td>1611788340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t1_gmd0xrl</td>\n      <td>1</td>\n      <td>t3_lehibh</td>\n      <td>1UBdU9GQvCnnXQHAcYaG1uL9V_U=</td>\n      <td>RedditSessions</td>\n      <td>1612683157</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t1_gggg1ed</td>\n      <td>1</td>\n      <td>t3_kgocvo</td>\n      <td>Io99IHkg-4QzX6xbKwbte0cuzp4=</td>\n      <td>memes</td>\n      <td>1608454223</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t1_g7zggfh</td>\n      <td>1</td>\n      <td>t3_j6n57d</td>\n      <td>EA1r-K5p_lVBLesLhCFRrKOPN-I=</td>\n      <td>videos</td>\n      <td>1602058834</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t1_fn060jg</td>\n      <td>26</td>\n      <td>t3_fyheuv</td>\n      <td>_aeNuqWD_AT5JIfooWYpKiZR8qg=</td>\n      <td>nfl</td>\n      <td>1586536065</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27997318</th>\n      <td>t1_f8sotb4</td>\n      <td>1</td>\n      <td>t3_e20bxb</td>\n      <td>EA1r-K5p_lVBLesLhCFRrKOPN-I=</td>\n      <td>Showerthoughts</td>\n      <td>1574785778</td>\n    </tr>\n    <tr>\n      <th>27997319</th>\n      <td>t1_eczq4gz</td>\n      <td>1</td>\n      <td>t3_abbin1</td>\n      <td>Pkz1m3vsliYpbnltUnWaQkPFLEo=</td>\n      <td>AskReddit</td>\n      <td>1546315274</td>\n    </tr>\n    <tr>\n      <th>27997320</th>\n      <td>t1_f240cz6</td>\n      <td>0</td>\n      <td>t3_dbujdv</td>\n      <td>NfwullBPKgqUPvj_Qr6RPnH1hrI=</td>\n      <td>DestinyTheGame</td>\n      <td>1569942007</td>\n    </tr>\n    <tr>\n      <th>27997321</th>\n      <td>t1_em778f3</td>\n      <td>2</td>\n      <td>t3_bj2bnd</td>\n      <td>kj12hcxGWPd3LxjKCNpoPFTDNBQ=</td>\n      <td>funny</td>\n      <td>1556683776</td>\n    </tr>\n    <tr>\n      <th>27997322</th>\n      <td>t1_efpy2wq</td>\n      <td>1</td>\n      <td>t3_amz190</td>\n      <td>vmXxzo7c6AP-ZOKZyiO3gxnxoOE=</td>\n      <td>memes</td>\n      <td>1549283237</td>\n    </tr>\n  </tbody>\n</table>\n<p>27997323 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data \n",
    "dataset = pd.read_csv('main_comments.csv.gz')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authors = dataset['author'].loc[~dataset['author'].isin(\n",
    "    ['EA1r-K5p_lVBLesLhCFRrKOPN-I=', 'Io99IHkg-4QzX6xbKwbte0cuzp4='])].unique()\n",
    "np.random.seed(0)\n",
    "sample_authors = np.random.choice(authors, 10000)\n",
    "sample_ds = dataset.loc[dataset['author'].isin(set(sample_authors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample some authors\n",
    "# Visualize user retention\n",
    "# Plot user retention rate (last action vs first action)\n",
    "start = sample_ds.groupby('author')['created_utc'].min()\n",
    "end = sample_ds.groupby('author')['created_utc'].max()\n",
    "diffs = end - start\n",
    "count = {}\n",
    "month = 2419200\n",
    "for diff in diffs:\n",
    "    num_months = diff // month\n",
    "    count[num_months] = count.get(num_months, 0) + 1\n",
    "count\n",
    "\n",
    "del count[0]\n",
    "\n",
    "# Visualization of how long the user stays on the platform.\n",
    "# It seems to decrease at an exponential rate.\n",
    "\n",
    "plt.bar(count.keys(), count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# We can try to cluster neighbors by similiar users\n",
    "np.random.seed(0)\n",
    "# users = np.random.choice(authors, 10000)\n",
    "users = np.random.choice(authors, 100000)\n",
    "sample_ds2 = dataset.loc[dataset['author'].isin(set(users))]\n",
    "sample_ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subreddits = sample_ds2['subreddit'].unique()\n",
    "subreddit_map = {}\n",
    "user_map = {}\n",
    "\n",
    "for idx, subreddit in enumerate(subreddits):\n",
    "    subreddit_map[subreddit] = idx\n",
    "for idx, user in enumerate(set(users)):\n",
    "    user_map[user] = idx\n",
    "\n",
    "matrix = [[0] * len(subreddits) for _ in range(len(user_map))]\n",
    "\n",
    "for row in sample_ds2.iloc():\n",
    "    matrix[user_map[row['author']]][subreddit_map[row['subreddit']]] = 1\n",
    "\n",
    "# n x d matrix for n users and d subreddits\n",
    "matrix = pd.DataFrame(matrix)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Factorize the matrix (embed subreddits into 2d space)\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(matrix)\n",
    "H = model.components_\n",
    "\n",
    "ask_reddit = 0\n",
    "memes = 0\n",
    "for i in range(len(subreddits)):\n",
    "    if subreddits[i] == 'AskReddit':\n",
    "        ask_reddit = i\n",
    "    elif subreddits[i] == 'memes':\n",
    "        memes = i\n",
    "\n",
    "H[0][ask_reddit] = 0\n",
    "H[1][ask_reddit] = 0\n",
    "H[0][memes] = 0\n",
    "H[1][memes] = 0\n",
    "\n",
    "plot = pd.DataFrame()\n",
    "plot['x'] = H[0]\n",
    "plot['y'] = H[1]\n",
    "\n",
    "# 2d embedding of how close subreddits are with some outliers removed\n",
    "fig = px.scatter(plot, x='x', y='y')\n",
    "fig.show('notebook')\n",
    "\n",
    "W2 = model.fit_transform(matrix.transpose())\n",
    "H2 = model.components_\n",
    "fig = px.scatter(x=H2[0], y=H2[1])\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# tf idf, properly weight matrix\n",
    "sum_mat = matrix.sum(axis=1)\n",
    "for i in range(len(sum_mat)):\n",
    "    sum_mat[i] = max(1, sum_mat[i])\n",
    "matrix2 = matrix.divide(sum_mat, axis=0)\n",
    "matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Factorize the matrix (embed subreddits into 2d space)\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(matrix)\n",
    "H = model.components_\n",
    "\n",
    "# get rid of ask reddit\n",
    "ask_reddit = 0\n",
    "memes = 0\n",
    "\n",
    "for i in range(len(subreddits)):\n",
    "    if subreddits[i] == 'AskReddit':\n",
    "        ask_reddit = i\n",
    "    elif subreddits[i] == 'memes':\n",
    "        memes = i\n",
    "\n",
    "H[0][ask_reddit] = 0\n",
    "H[1][ask_reddit] = 0\n",
    "H[0][memes] = 0\n",
    "H[1][memes] = 0\n",
    "\n",
    "plot = pd.DataFrame()\n",
    "plot['x'] = H[0]\n",
    "plot['y'] = H[1]\n",
    "plot['subreddit'] = subreddits\n",
    "\n",
    "# 2d embedding of how close subreddits are with some outliers removed\n",
    "# Matrix uses additional tf idf weights to adjust for users subscribed to multiple\n",
    "# Subreddits\n",
    "fig = px.scatter(plot, x='x', y='y')\n",
    "fig.show('notebook')\n",
    "\n",
    "W = model.fit_transform(matrix.transpose())\n",
    "H = model.components_\n",
    "fig = px.scatter(plot, x=H[0], y=H[1])\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Let us visualize this with pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_mat = pca.fit_transform(matrix2)\n",
    "fig = px.scatter(x=pca_mat[:, 0], y=pca_mat[:, 1])\n",
    "fig.show()\n",
    "\n",
    "pca_mat = pca.fit_transform(matrix)\n",
    "fig = px.scatter(x=pca_mat[:, 0], y=pca_mat[:, 1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Show distrubution of users\n",
    "user_distribution = sample_ds2['subreddit'].value_counts()\n",
    "user_distribution.plot(logy=True, title='User distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = sample_ds2.groupby('author')['created_utc'].min()\n",
    "end = sample_ds2.groupby('author')['created_utc'].max()\n",
    "Y = end - start\n",
    "X = matrix2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.33, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(trainX, trainY)\n",
    "reg.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = len(testX)\n",
    "# View regression on plan\n",
    "y = reg.predict(testX)\n",
    "\n",
    "# Pca onto one dimension\n",
    "pca = PCA(n_components=1)\n",
    "x = pca.fit_transform(testX).reshape(n)\n",
    "\n",
    "\n",
    "plotGraph = pd.DataFrame()\n",
    "plotGraph['x'] = np.concatenate((x, x))\n",
    "plotGraph['y'] = np.concatenate((y, testY.values.reshape(n)))\n",
    "plotGraph['color'] = [0] * n + [1] * n\n",
    "plotGraph['hover'] = ['prediction'] * n + ['actual'] * n\n",
    "\n",
    "fig = px.scatter(plotGraph, \n",
    "                 x='x', \n",
    "                 y='y', \n",
    "                 color='color', \n",
    "                 log_y=True, \n",
    "                 hover_data=['hover'],\n",
    "                 title='Regression Plot testing'\n",
    "                )\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n = len(trainX)\n",
    "# Same plot with training data\n",
    "y = reg.predict(trainX)\n",
    "\n",
    "# Pca onto one dimension\n",
    "pca = PCA(n_components=1)\n",
    "x = pca.fit_transform(trainX).reshape(n)\n",
    "\n",
    "plotGraph = pd.DataFrame()\n",
    "plotGraph['x'] = np.concatenate((x, x))\n",
    "plotGraph['y'] = np.concatenate((y, trainY.values.reshape(n)))\n",
    "plotGraph['color'] = [0] * n + [1] * n\n",
    "plotGraph['hover'] = ['prediction'] * n + ['actual'] * n\n",
    "\n",
    "fig = px.scatter(\n",
    "    plotGraph, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    color='color', \n",
    "    log_y=True, \n",
    "    hover_data=['hover'], \n",
    "    title='Regression Plot on Training')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let us revisit the plots before and see if certain clusters share retency levels\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_mat = pca.fit_transform(matrix)\n",
    "\n",
    "plot = pd.DataFrame()\n",
    "plot['x'] = pca_mat[:, 0]\n",
    "plot['y'] = pca_mat[:, 1]\n",
    "plot['color'] = diff.values\n",
    "fig = px.scatter(plot, x='x', y='y', color='color')\n",
    "fig.show()\n",
    "\n",
    "pca_mat2 = pca.fit_transform(matrix2)\n",
    "plot2 = pd.DataFrame()\n",
    "plot2['x'] = pca_mat2[:, 0]\n",
    "plot2['y'] = pca_mat2[:, 1]\n",
    "plot2['color'] = diff.values\n",
    "fig2 = px.scatter(plot2, x='x', y='y', color='color')\n",
    "fig2.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's view the plot with what our regression thinks the result is\n",
    "c1 = reg.predict(matrix)\n",
    "c2 = reg.predict(matrix2)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_mat = pca.fit_transform(matrix)\n",
    "plot = pd.DataFrame()\n",
    "plot['x'] = pca_mat[:, 0]\n",
    "plot['y'] = pca_mat[:, 1]\n",
    "plot['color'] = c1\n",
    "fig = px.scatter(plot, x='x', y='y', color='color')\n",
    "fig.show()\n",
    "\n",
    "pca_mat2 = pca.fit_transform(matrix2)\n",
    "plot2 = pd.DataFrame()\n",
    "plot2['x'] = pca_mat2[:, 0]\n",
    "plot2['y'] = pca_mat2[:, 1]\n",
    "plot2['color'] = c2\n",
    "fig2 = px.scatter(plot2, x='x', y='y', color='color')\n",
    "fig2.show()\n",
    "\n",
    "# It seems, that for each cluster has a humogonous distrbution, are there are far fewer zeroes in our regression\n",
    "# plot, maybe we should see the results if we remove our zeros"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inverse_subreddit = {}\n",
    "\n",
    "# Filter out 0s\n",
    "filtr = list(~(diff==0))\n",
    "X = matrix2[filtr]\n",
    "Y = diff[filtr]\n",
    "\n",
    "# Get subreddits list\n",
    "for subreddit in subreddit_map:\n",
    "    inverse_subreddit[subreddit_map[subreddit]] = subreddit\n",
    "\n",
    "def transform_row(row):\n",
    "    subreddits = []\n",
    "    for i in range(len(row)):\n",
    "        if row[i] > 0:\n",
    "            subreddits.append(inverse_subreddit[i])\n",
    "    return subreddits\n",
    "reddits = X.transpose().apply(transform_row)\n",
    "\n",
    "# Train new regressor\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.33, random_state=0)\n",
    "reg = LinearRegression().fit(trainX, trainY)\n",
    "\n",
    "\n",
    "pca_mat2 = pca.fit_transform(X)\n",
    "plot2 = pd.DataFrame()\n",
    "plot2['x'] = pca_mat2[:, 0]\n",
    "plot2['y'] = pca_mat2[:, 1]\n",
    "plot2['color'] = Y.values\n",
    "plot2['reddits'] = reddits\n",
    "fig2 = px.scatter(plot2, x='x', y='y', color='color', hover_data=['reddits'])\n",
    "fig2.show()\n",
    "\n",
    "plot3 = pd.DataFrame()\n",
    "plot3['x'] = pca_mat2[:, 0]\n",
    "plot3['y'] = pca_mat2[:, 1]\n",
    "plot3['color'] = reg.predict(X)\n",
    "plot3['reddits'] = reddits\n",
    "fig3 = px.scatter(plot3, x='x', y='y', color='color', hover_data=['reddits'])\n",
    "fig3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_subreddit = {}\n",
    "\n",
    "# Filter out 0s\n",
    "filtr = list(~(diff==0))\n",
    "X = matrix2[filtr]\n",
    "Y = diff[filtr]\n",
    "\n",
    "# Get subreddits list\n",
    "for subreddit in subreddit_map:\n",
    "    inverse_subreddit[subreddit_map[subreddit]] = subreddit\n",
    "\n",
    "def transform_row(row):\n",
    "    subreddits = []\n",
    "    for i in range(len(row)):\n",
    "        if row[i] > 0:\n",
    "            subreddits.append(inverse_subreddit[i])\n",
    "    return subreddits\n",
    "reddits = X.transpose().apply(transform_row)\n",
    "\n",
    "# Train new regressor\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.33, random_state=0)\n",
    "reg = LinearRegression().fit(trainX, trainY)\n",
    "\n",
    "\n",
    "pca_mat2 = pca.fit_transform(X)\n",
    "plot2 = pd.DataFrame()\n",
    "plot2['x'] = pca_mat2[:, 0]\n",
    "plot2['y'] = pca_mat2[:, 1]\n",
    "plot2['color'] = Y.values\n",
    "plot2['reddits'] = reddits\n",
    "fig2 = px.scatter(plot2, x='x', y='y', color='color', hover_data=['reddits'])\n",
    "fig2.show()\n",
    "\n",
    "plot3 = pd.DataFrame()\n",
    "plot3['x'] = pca_mat2[:, 0]\n",
    "plot3['y'] = pca_mat2[:, 1]\n",
    "plot3['color'] = reg.predict(X)\n",
    "plot3['reddits'] = reddits\n",
    "fig3 = px.scatter(plot3, x='x', y='y', color='color', hover_data=['reddits'])\n",
    "fig3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}